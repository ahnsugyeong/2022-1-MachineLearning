{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T09:17:01.506351Z","iopub.execute_input":"2022-04-29T09:17:01.507282Z","iopub.status.idle":"2022-04-29T09:17:01.525081Z","shell.execute_reply.started":"2022-04-29T09:17:01.507238Z","shell.execute_reply":"2022-04-29T09:17:01.523997Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.526677Z","iopub.execute_input":"2022-04-29T09:17:01.527022Z","iopub.status.idle":"2022-04-29T09:17:01.531342Z","shell.execute_reply.started":"2022-04-29T09:17:01.526984Z","shell.execute_reply":"2022-04-29T09:17:01.530714Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv('../input/2022-ml-midterm-p2/submit.csv')\ntest = pd.read_csv('../input/2022-ml-midterm-p2/test.csv')\ntrain = pd.read_csv('../input/2022-ml-midterm-p2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.744506Z","iopub.execute_input":"2022-04-29T09:17:01.744965Z","iopub.status.idle":"2022-04-29T09:17:01.771263Z","shell.execute_reply.started":"2022-04-29T09:17:01.744927Z","shell.execute_reply":"2022-04-29T09:17:01.770585Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"train.head()\ntrainX = train.drop(['label', 'index'], axis = 1)\ntest = test.drop(['index'], axis = 1)\ntrainY = train['label']","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.772779Z","iopub.execute_input":"2022-04-29T09:17:01.773170Z","iopub.status.idle":"2022-04-29T09:17:01.779509Z","shell.execute_reply.started":"2022-04-29T09:17:01.773121Z","shell.execute_reply":"2022-04-29T09:17:01.778885Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"# c = trainX.columns\n\n# for i in c:\n#     print(len(trainX[i].unique()))\n#     if len(trainX[i].unique()) <= 1 :\n#         trainX = trainX.drop([i], axis = 1)\n#         print('droped')\n#         test = test.drop([i], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.780586Z","iopub.execute_input":"2022-04-29T09:17:01.781172Z","iopub.status.idle":"2022-04-29T09:17:01.794598Z","shell.execute_reply.started":"2022-04-29T09:17:01.781137Z","shell.execute_reply":"2022-04-29T09:17:01.793703Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.796101Z","iopub.execute_input":"2022-04-29T09:17:01.796331Z","iopub.status.idle":"2022-04-29T09:17:01.809963Z","shell.execute_reply.started":"2022-04-29T09:17:01.796305Z","shell.execute_reply":"2022-04-29T09:17:01.809312Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import RobustScaler\n# scaler = RobustScaler()\n# trainX_sc = scaler.fit_transform(trainX)\n# test_sc = scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.811886Z","iopub.execute_input":"2022-04-29T09:17:01.812345Z","iopub.status.idle":"2022-04-29T09:17:01.822779Z","shell.execute_reply.started":"2022-04-29T09:17:01.812303Z","shell.execute_reply":"2022-04-29T09:17:01.821954Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.824140Z","iopub.execute_input":"2022-04-29T09:17:01.824631Z","iopub.status.idle":"2022-04-29T09:17:01.837069Z","shell.execute_reply.started":"2022-04-29T09:17:01.824598Z","shell.execute_reply":"2022-04-29T09:17:01.836368Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# clf = QuadraticDiscriminantAnalysis()\n# clf.fit(trainX, trainY)\n# pred = clf.predict(test)\n\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# clf = LinearDiscriminantAnalysis()\n# clf.fit(trainX, trainY)\n# trainX_lda = clf.transform(trainX)\n# test_lda = clf.transform(test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=6, weights = 'distance')\n#print(neigh.score(X_test, y_test))\nneigh.fit(X_train, y_train)\npred = neigh.predict(test)\n\n\n# from sklearn.ensemble import RandomForestClassifier\n# clf = RandomForestClassifier( random_state=42)\n# clf.fit(trainX, trainY)\n# print(clf.score(trainX, trainY))\n# pred = clf.predict(test)\n\n# from sklearn.linear_model import LogisticRegression\n# clf = LogisticRegression(random_state=42, C = 200, max_iter = 100000)\n# clf.fit(trainX, trainY)\n# print(clf.score(trainX, trainY))\n# pred = clf.predict(test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.838167Z","iopub.execute_input":"2022-04-29T09:17:01.838779Z","iopub.status.idle":"2022-04-29T09:17:01.869559Z","shell.execute_reply.started":"2022-04-29T09:17:01.838738Z","shell.execute_reply":"2022-04-29T09:17:01.868547Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"sample['label'] = pred\nsample.to_csv('submission.csv', index = False)\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T09:17:01.871023Z","iopub.execute_input":"2022-04-29T09:17:01.871791Z","iopub.status.idle":"2022-04-29T09:17:01.897830Z","shell.execute_reply.started":"2022-04-29T09:17:01.871756Z","shell.execute_reply":"2022-04-29T09:17:01.896858Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"# Review\n\n다양한 방법론을 시도해봤지만 baseline을 못 넘겼던 문제..\n내 실수는 drop에서 있었다. 모델 학습에 영향을 미치지 않도록 label뿐만 아니라 index도 drop을 했어야했는데\n그 부분에서 성능 차이가 심하게 났다.\nbaseline은 KNeighborsClassifier(n_neighbors = 8) 정도로 나왔고\ntrain_test_split를 통하여 0.98888까지 올릴 수 있었다.\n\n문제를 처음 읽을 때 차분하게 올바른 판단을 내려야하는데\n시험 상황에서 마음이 급해서 너무 우왕좌왕했다.\n\n나머지 문제를 생각하기보단 지금 풀고있는 문제에 집중할 것! 🥹","metadata":{}}]}