{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"execution":{"iopub.status.busy":"2022-06-06T07:30:16.631356Z","iopub.execute_input":"2022-06-06T07:30:16.631635Z","iopub.status.idle":"2022-06-06T07:30:17.46078Z","shell.execute_reply.started":"2022-06-06T07:30:16.631604Z","shell.execute_reply":"2022-06-06T07:30:17.459739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tqdm\nimport time\nimport cv2\nimport pickle\nimport torch\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.svm import SVC\nfrom scipy.stats import mode\nfrom sklearn.decomposition import PCA\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.462671Z","iopub.execute_input":"2022-06-06T07:30:17.463571Z","iopub.status.idle":"2022-06-06T07:30:17.469071Z","shell.execute_reply.started":"2022-06-06T07:30:17.463521Z","shell.execute_reply":"2022-06-06T07:30:17.468359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"비디오 : 2020개\n\n프레임(이미지) : 10100개\n\n이미지당 SIFT : 256개\n\n군집의 갯수 : 5개\n\n----------------\n\n새로운 이미지 SIFT : 256개\n\n\n40개 : 0번째\n\n30개 : 1번째\n\n50개 : 2번째 \n\n[40,30,50,60,...] : BoVW \n\n\n\n","metadata":{}},{"cell_type":"code","source":"# 베이스라인 달성을 위한 파라미터 제공\narg_img_size = (128, 128)\narg_dense_sift = True\nargs_local_cluster = 200\nargs_global_cluster = 200\nnum_frame = 5\n\n#############################\n# VLAD와 BoW 중 어떤 방식을 사용할 지 결정해줍니다.\n#############################\nargs_aggr = \"vlad\" # or \"vlad\"\n\npca_vlad = 128","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.470431Z","iopub.execute_input":"2022-06-06T07:30:17.470756Z","iopub.status.idle":"2022-06-06T07:30:17.491867Z","shell.execute_reply.started":"2022-06-06T07:30:17.470715Z","shell.execute_reply":"2022-06-06T07:30:17.490724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 비디오 전처리 및 프레임 별 특징점(visual word) 추출 (Empty Module 1)","metadata":{}},{"cell_type":"code","source":"# train 비디오의 행동 분류 label read\nroot = \"/kaggle/input/2022-ml-project5/\"\ntrain_csv = os.path.join(root, \"train_label.csv\")\ntrain_csv = pd.read_csv(train_csv)\ntrain_csv_arr = np.asarray(train_csv)\n\n# 데이터 셋에 존재하는 행동 분류 정보 read\nclassinfo = os.path.join(root, \"class_info.csv\")\nclassinfo = pd.read_csv(classinfo)\nclassinfo_arr = np.asarray(classinfo)\n\n\ntrain_path = os.path.join(root, \"train\")\ntest_path = os.path.join(root, \"test\")\n\n# train 비디오 경로\ntrain_list = os.listdir(train_path)\ntrain_list.sort()\ntrain_list = [os.path.join(train_path, i) for i in train_list]\n\n# test 비디오 경로\ntest_list = os.listdir(test_path)\ntest_list.sort()\ntest_list = [os.path.join(test_path, i) for i in test_list]","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.493901Z","iopub.execute_input":"2022-06-06T07:30:17.494132Z","iopub.status.idle":"2022-06-06T07:30:17.518231Z","shell.execute_reply.started":"2022-06-06T07:30:17.494105Z","shell.execute_reply":"2022-06-06T07:30:17.517277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def video_to_frame(video_path, size, num_frame):\n    \n    #########################################################\n    ## 비디오에서 프레임을 추출해주는 함수\n    ## \n    ## Input \n    ##     video_path : 한 비디오의 경로\n    ##     size : 비디오 내의 프레임을 읽을 때, 원하는 해상도 크기\n    ##     num_frames : 한 비디오 내에서 읽을 프레임의 수\n    ##\n    ## Output\n    ##     frames : 읽고 저장한 총 프레임\n    #########################################################\n    \n    cap = cv2.VideoCapture(video_path)\n    \n    # 한 비디오의 총 프레임 수 반환 및 원하는 프레임 수 많큼 읽기 위해 읽을 프레임의 인덱스 설정\n    total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    sel_ind = np.linspace(0, total_frame-1, num_frame).astype(\"int\")\n    \n    num=0\n    frames = []\n    for i in range(total_frame): \n        # 읽을 프레임 인덱스의 경우 프레임 읽어 메모리에 저장, 아닐 경우 지나감\n        if i in sel_ind:\n            res, frame = cap.read()\n            # 원하는 해상도로 조절 및 grayscale로 변환\n            frame = cv2.resize(frame, size, interpolation = cv2.INTER_CUBIC)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            frames.append(frame)\n        else:\n            res = cap.grab()        \n    cap.release()\n    frames = np.asarray(frames)\n\n    return frames","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.520624Z","iopub.execute_input":"2022-06-06T07:30:17.520887Z","iopub.status.idle":"2022-06-06T07:30:17.528097Z","shell.execute_reply.started":"2022-06-06T07:30:17.520856Z","shell.execute_reply":"2022-06-06T07:30:17.527249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n####### Empty Module 1 : DenseSIFT ########\n            # 기본 SIFT 와 동일하게 정의 \n            # 기본 SIFT 에서는 detectAndCompute를 사용했지만 \n            # Dense SIFT는 위에서 생성한 keypoint를 사용해 compute 만을 진행 \n            #\n            # 참고) cv2.SIFT_create() 함수와 .compute()함수를 이용하여 구현한다.\n            ###########################################\n            \n                \n    #########################################################\n    ## 비디오 내의 프레임에서 특징점(visual word)을 SIFT or DenseSIFT로 추출해주는 함수\n    ## \n    ## Input \n    ##     data : 한 비디오에서 읽고 저장한 프레임\n    ##     dense : SIFT or DenseSIFT 사용 여부\n    ##\n    ## Output\n    ##     x : 프레임에 대해 추출된 특징점(visual word), dict 형태 -> x[0]이면 0번째 인덱스 프레임의 특징점(visual word) [n,128] 확인 가능\n    #########################################################\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.52953Z","iopub.execute_input":"2022-06-06T07:30:17.529776Z","iopub.status.idle":"2022-06-06T07:30:17.546017Z","shell.execute_reply.started":"2022-06-06T07:30:17.529745Z","shell.execute_reply":"2022-06-06T07:30:17.545176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def computeSIFT(data, dense=False):\n    x = {}\n    for i in range(0, len(data)):\n        if dense: # DenseSIFT 추출\n            img = data[i]\n            step_size = 8\n            kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size) for y in range(0, img.shape[1], step_size)] # keypoint 생성\n            sift = cv2.SIFT_create()\n            kp, desc = sift.compute(img, kp) # Dense SIFT는 위에서 생성한 keypoint를 사용해 compute 만을 진행 \n        \n        else: # 기본 SIFT 추출\n            sift = cv2.SIFT_create()\n            img = data[i]\n            kp, desc = sift.detectAndCompute(img, None)\n        x.update({i : desc})\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.547535Z","iopub.execute_input":"2022-06-06T07:30:17.547795Z","iopub.status.idle":"2022-06-06T07:30:17.564542Z","shell.execute_reply.started":"2022-06-06T07:30:17.547762Z","shell.execute_reply":"2022-06-06T07:30:17.563615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 train_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\ntrain_local_desc = {}\nfor vi, vid_path in enumerate(tqdm.tqdm(train_list, desc=\"Extract {} in train data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n    train_local_desc.update({vid_path : local_desc})\n\n# test 비디오에서 프레임 추출 및 특징점(visual word) 추출, dict 형태로 test_local_desc[비디오 경로]이면 해당하는 비디오에서 추출한 모든 특징점(visual word) 확인 가능\ntest_local_desc = {}\nfor vi, vid_path in enumerate(tqdm.tqdm(test_list, desc=\"Extract {} in test data\".format(\"dsift\" if arg_dense_sift else \"sift\"))):\n    curr_frame = video_to_frame(vid_path, arg_img_size, num_frame)\n    local_desc = computeSIFT(curr_frame, arg_dense_sift)\n    test_local_desc.update({vid_path : local_desc})\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:30:17.56602Z","iopub.execute_input":"2022-06-06T07:30:17.566253Z","iopub.status.idle":"2022-06-06T07:32:38.499295Z","shell.execute_reply.started":"2022-06-06T07:30:17.566225Z","shell.execute_reply":"2022-06-06T07:32:38.497697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nAggregate SIFT descriptor\")\nstart = time.time()\n\n\n# train 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n# 같은 인덱스의 train_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 train_local_info에서 확인 가능\ntrain_frame_total = []\ntrain_local_desc_total = []\ntrain_local_info = {}\nfor k, v in train_local_desc.items():\n    for kk, vv in v.items():\n        l_num = 0\n        if vv is not None:\n            train_local_desc_total.extend(vv)\n            train_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n            l_num = len(vv)\n        train_local_info.update({k+\", \"+str(kk) : l_num})\ntrain_local_desc_total = np.asarray(train_local_desc_total)\ntrain_frame_total = np.asarray(train_frame_total)\n\n\n# test 비디오 별로 나눠진 특징점(visual word)들을 [n,128]형태로 모음, 모아진 특징점(visual word)들의 정보(비디오 내의 몇번째 프레임에서 나온 특징점인지)는 \n# 같은 인덱스의 test_frame_total에서 확인 가능 및 비디오 내의 특정 프레임에서 나온 특징점(visual word)의 수는 test_local_info에서 확인 가능\ntest_frame_total = []\ntest_local_desc_total = []\ntest_local_info = {}\nfor k, v in test_local_desc.items():\n    for kk, vv in v.items():\n        l_num = 0\n        if vv is not None:\n            test_local_desc_total.extend(vv)\n            test_frame_total.extend([k+\", \"+str(kk)] * len(vv))\n            l_num = len(vv)\n        test_local_info.update({k+\", \"+str(kk) : l_num})\ntest_local_desc_total = np.asarray(test_local_desc_total)\ntest_frame_total = np.asarray(test_frame_total)\n\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:38.500736Z","iopub.execute_input":"2022-06-06T07:32:38.501109Z","iopub.status.idle":"2022-06-06T07:32:42.200596Z","shell.execute_reply.started":"2022-06-06T07:32:38.501067Z","shell.execute_reply":"2022-06-06T07:32:42.199737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 비디오 feature 기술을 위한 프레임 feature 기술 (Empty Module 2,3,4)","metadata":{}},{"cell_type":"code","source":"    #########################################################\n    ## 모든 특징점들 중, 대표 특징점(codebook)을 선정하는 함수\n    ## \n    ## Input \n    ##     train_desc : 모든 train 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n    ##     test_desc : 모든 test 비디오의 모든 프레임에서 추출한 특징점(visual word)들\n    ##     n_clusters : 대표 특징점(codebook)의 수\n    ##\n    ## Output\n    ##     train_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n    ##     test_pred : 대표 특징점(codebook)에 대해 train_desc가 할당된 위치\n    ##     clusters : 대표 특징점(codebook)\n    ##     kmeans : kmeans 인스턴스\n    #########################################################\n    \n    \n    ##### Empty Module 2 : 대표 특징점(codebook) 선정 ######\n    # 제약 조건 : MiniBatchKMeans 사용, random_state=0 고정\n    # sklearn의 MiniBatchKMeans를 활용하여 n_clusters 크기 만큼의 대표 특징점(codebook)을 선정\n    ###########################################","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:42.203976Z","iopub.execute_input":"2022-06-06T07:32:42.204275Z","iopub.status.idle":"2022-06-06T07:32:42.209418Z","shell.execute_reply.started":"2022-06-06T07:32:42.204241Z","shell.execute_reply":"2022-06-06T07:32:42.208443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clustering(train_desc, test_desc=None, n_clusters=200):\n    from sklearn.cluster import MiniBatchKMeans\n    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state = 0).fit(train_desc) # 군집화 알고리즘인 MiniBatchKMeans를 사용하여 n_clusters개의 대표 특징점 생성\n    clusters = kmeans.cluster_centers_ # 대표 특징점 선정\n    train_pred = kmeans.predict(train_desc)\n    \n    if test_desc is not None:\n        test_pred = kmeans.predict(test_desc)\n    else:\n        test_pred = None\n    return train_pred, test_pred, clusters, kmeans","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:42.210537Z","iopub.execute_input":"2022-06-06T07:32:42.210781Z","iopub.status.idle":"2022-06-06T07:32:42.227722Z","shell.execute_reply.started":"2022-06-06T07:32:42.210751Z","shell.execute_reply":"2022-06-06T07:32:42.226607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 모든 train 비디오의 모든 프레임에서 추출한 특징점(visual word)들로 대표 특징점(codebook)을 만들고,\n# train 비디오의 모든 프레임에서 추출된 특징점(visual word)과 test 비디오의 모든 프레임에서 추출된 특징점(visual word)을 대표 특징점(codebook)에 할당\ntrain_local_alloc, test_local_alloc, local_codebook, local_kmeans = clustering(train_local_desc_total, test_local_desc_total, args_local_cluster)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-06T07:32:42.230484Z","iopub.execute_input":"2022-06-06T07:32:42.230849Z","iopub.status.idle":"2022-06-06T07:32:54.970006Z","shell.execute_reply.started":"2022-06-06T07:32:42.230811Z","shell.execute_reply":"2022-06-06T07:32:54.969221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BoVW (Bag of Visual Word) 및 VLAD (Vector of Locally Aggregated Descriptors)","metadata":{}},{"cell_type":"code","source":"#########################################################\n    ## 이미지 feature인 VLAD feature를 기술하기 위한 함수\n    ## \n    ## Input \n    ##     X : 한 프레임의 특징점(visual word)들\n    ##     alloc : 한 프레임의 특징점(visual word)들이 대표 특징점(codebook)에 할당된 위치\n    ##     centers : 대표 특징점(codebook)\n    ##\n    ## Output\n    ##     V : VLAD feature\n    #########################################################\\\n####################### Empty Module 3 : VLAD ########################\n            # 이미지에서 추출된 특징점(visual word) X와 이들이 대표 특징점(codebook)으로 할당된 정보 alloc을 이용해,\n            # 동일한 대표 특징점(codebook)으로 할당된 특징점(visual word)들의 벡터 합 계산해 V[i]에 저장\n            # hint : 바로 위 조건문의 조건 \"alloc==i\"의 의미를 파악하고 인덱싱에 활용\n            ######################################################################\n            \n            #########################################################\n    ## 이미지 feature인 BoW feature를 기술하기 위한 함수\n    ## \n    ## Input \n    ##     alloc : 한 프레임의 특징점(visual word)들이 대표 특징점(codebook)에 할당된 위치\n    ##     n_cluster : 대표 특징점(codebook)의 수\n    ##\n    ## Output\n    ##     V : BoW feature\n    #########################################################\n    \n    ######################### Empty Module 4 : BoW ##########################\n    # 이미지에서 추출된 특징점(visual word)이 대표 특징점(codebook)으로 할당된 정보 alloc의 histogram을 계산\n    # np.histogram 함수 참고\n    #########################################################################\n    #import pdb;pdb.set_trace()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:54.97118Z","iopub.execute_input":"2022-06-06T07:32:54.971559Z","iopub.status.idle":"2022-06-06T07:32:54.976353Z","shell.execute_reply.started":"2022-06-06T07:32:54.971528Z","shell.execute_reply":"2022-06-06T07:32:54.975486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def VLAD(X, alloc, centers):\n    m,d = X.shape\n    k = centers.shape[0]\n    # VLAD feature를 담기 위한 변수\n    V=np.zeros([k,d]) \n    # 이미지에서 추출된 특징점(visual word) X와 이들이 대표 특징점(codebook)으로 할당된 정보 alloc을 이용해 \n    # 동일한 대표 특징점(codebook)으로 할당된 특징점(visual word)들의 벡터 합 계산해 V[i]에 저장\n    for i in range(k): \n        if np.sum(alloc==i)>0:\n            V[i]=np.sum(X[alloc==i,:]-centers[i], axis=0)\n    # 후처리 과정\n    V = V.flatten()\n    V = np.sign(V)*np.sqrt(np.abs(V))\n    if np.sqrt(np.dot(V,V))!=0:\n        V = V/np.sqrt(np.dot(V,V))\n    return V\n\ndef BoW(alloc, n_cluster):\n    # VLAD feature를 담기 위한 변수\n    V, edge = np.histogram(alloc,bins = range(n_cluster+1), normed = True) # 이미지에서 추출된 특징점(visual word)이 대표 특징점(codebook)으로 할당된 정보 alloc의 histogram을 계산\n    return V","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:54.977319Z","iopub.execute_input":"2022-06-06T07:32:54.97751Z","iopub.status.idle":"2022-06-06T07:32:54.9945Z","shell.execute_reply.started":"2022-06-06T07:32:54.977485Z","shell.execute_reply":"2022-06-06T07:32:54.99369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 비디오 내 프레임 별로 이미지 feature 기술","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\nAllocate center & Descript local histogram\")\nstart = time.time()\n\n# Train 비디오 내의 프레임 별로 이미지 feature 기술 -> train_global_desc\n# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> train_global_desc_key\ntrain_global_desc = []\ntrain_global_desc_key = []\nvi=0\nfor k, v in train_local_info.items():\n    if v!=0:\n        if args_aggr==\"bow\":            \n            hist_desc = BoW(train_local_alloc[vi:vi+v], args_local_cluster\n                           )\n        elif args_aggr==\"vlad\":\n            hist_desc = VLAD(train_local_desc_total[vi:vi+v], train_local_alloc[vi:vi+v], local_codebook)\n        else:\n            print(\"Set the appropriate variable args_aggr!!\")\n            import pdb; pdb.set_trace()\n\n        vi+=v\n        train_global_desc.append(hist_desc)\n        train_global_desc_key.append(k)\ntrain_global_desc = np.asarray(train_global_desc)\ntrain_global_desc_key = np.asarray(train_global_desc_key)\n\n\n# Test 비디오 내의 프레임 별로 이미지 feature 기술 -> test_global_desc\n# 각 이미지 feature의 정보(속한 비디오 이름, 비디오 내의 인덱스) -> test_global_desc_key\ntest_global_desc = []\ntest_global_desc_key = []\nvi=0\nfor k, v in test_local_info.items():\n    if v!=0:\n        if args_aggr==\"bow\":\n            hist_desc = BoW(test_local_alloc[vi:vi+v], args_local_cluster)\n        elif args_aggr==\"vlad\":\n            hist_desc = VLAD(test_local_desc_total[vi:vi+v], test_local_alloc[vi:vi+v], local_codebook)\n        else:\n            print(\"Set the appropriate variable args_aggr!!\")\n            import pdb; pdb.set_trace()\n\n        vi+=v\n        test_global_desc.append(hist_desc)\n        test_global_desc_key.append(k)\ntest_global_desc = np.asarray(test_global_desc)\ntest_global_desc_key = np.asarray(test_global_desc_key)\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:32:54.99614Z","iopub.execute_input":"2022-06-06T07:32:54.996747Z","iopub.status.idle":"2022-06-06T07:34:02.131683Z","shell.execute_reply.started":"2022-06-06T07:32:54.996705Z","shell.execute_reply":"2022-06-06T07:34:02.130619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VLAD feature의 경우 큰 차원으로 인해 메모리 부족 현상이 발생하므로 PCA를 이용한 차원 축소\nif args_aggr==\"vlad\":\n    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n    start = time.time()\n    pca = PCA(n_components=pca_vlad, random_state=0)\n    pca.fit(train_global_desc)\n    train_global_desc = pca.transform(train_global_desc)\n    test_global_desc = pca.transform(test_global_desc)\n    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T07:34:02.132966Z","iopub.execute_input":"2022-06-06T07:34:02.133188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nProcessing label\")\nstart = time.time()\n\n# 분류를 위해, 행동 분류에 대한 train 비디오의 각 프레임 별 label 가공\ntrain_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_global_desc_key])\ntrain_global_label = []\nfor fid in train_global_id:\n    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n    clsname = train_csv_arr[cind, 1]\n    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n    train_global_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\ntrain_global_label = np.asarray(train_global_label).ravel()\n\n# 분류를 위해, 행동 분류에 대한 test 비디오의 각 프레임 별 id 가공 \ntest_global_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_global_desc_key])\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveFile(predict, predict_id, name, best_params=None):\n    #########################################################\n    ## 결과를 저장하기 위한 함수\n    ## \n    ## Input \n    ##     predict : 모든 test 비디오의 행동 예측 값\n    ##     predict_id : 모든 test 비디오의 id\n    ##     name : 원하는 저장 파일 이름\n    ##     best_params : 원하는 instance 저장 시 사용\n    ##\n    #########################################################\n    \n    data = np.concatenate((np.expand_dims(predict_id.astype(\"str\"), axis=1), np.expand_dims(predict.astype(\"str\"), axis=1)), axis=1)\n    csv = pd.DataFrame(data, columns=['Id', 'Category'])\n    csv.to_csv(name+'.csv', index = False)\n    if name == 'svm_global_averaging': csv.to_csv(\"submission.csv\", index=False)\n    \n    if best_params:\n        f = open(name + \".pickle\", \"wb\")\n        pickle.dump(best_params, f, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 비디오의 모든 프레임에서 얻은 feature를 평균내어 비디오 feature 생성 및 분류 (Empty Module 5)","metadata":{}},{"cell_type":"code","source":"#################### Empty Module 5 : SVM (averaging) ######################\n# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n# 각 프레임을 나타내는 이미지 feature를 비디오 별로 평균 내어 비디오 feature로 사용\n# 비디오 feature로 학습 후, 행동 예측\n# hint : 위 셀에서 선언한 train_global_id, train_global_label, test_global_id 및 np.mean() 활용\n###########################################################################","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nSVM global averaging in frame\")\nstart = time.time()\n\ntrain_avg_video_desc = []\ntrain_avg_video_label = []\n\n# feature별로 평균 구하여 저장\nfor i in np.unique(train_global_id): # train_global_id\n    tind = np.where(train_global_id==i)[0]\n    train_avg_video_desc.append(np.mean(train_global_desc[tind], axis = 0))\n    train_avg_video_label.append(np.mean(train_global_label[tind]))\ntrain_avg_video_desc = np.asarray(train_avg_video_desc)\ntrain_avg_video_label = np.asarray(train_avg_video_label)\n\ntest_avg_video_desc = []\nfor i in np.unique(test_global_id):\n    tind = np.where(test_global_id==i)[0]\n    test_avg_video_desc.append(np.mean(test_global_desc[tind], axis = 0))\ntest_avg_video_desc = np.asarray(test_avg_video_desc)\n\n# support vector machine classifier\nfrom sklearn.svm import SVC\nclf = SVC(random_state = 0) # baseline : 0.27920\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100) # 0.29108\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.5) # 0.29702\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.55) # 0.29108\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.45) # 0.29900\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 1, tol = 0.45) # 0.27524\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.3) # 0.29702\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.35) # 0.29108\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.47) # 0.29900\nclf = SVC(random_state = 0, class_weight = 'balanced', probability = True, C = 100, tol = 0.445) # 0.29900 (best)\n\n# 분류기로 학습 및 예측\nclf.fit(train_avg_video_desc, train_avg_video_label)\nsvm_predict = clf.predict(test_avg_video_desc).astype('int64')\n\nsaveFile(classinfo_arr[svm_predict][:,1], np.arange(len(test_list)), \"svm_global_averaging\")\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 비디오의 모든 프레임에서 얻은 feature로 분류기를 사용해 예측하고 예측치 중 가장 많은 빈도를 나타낸 행동을 선정 (Empty Module 6)","metadata":{}},{"cell_type":"code","source":"############################### Empty Module 6 : SVM (voting) ##################################\n# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n# 각 프레임을 나타내는 이미지 feature를 모두 사용해 SVM 학습\n# 학습 시, 각 이미지 feature의 label은 해당되는 비디오의 label(행동)로 사용\n# 프레임 별로 행동 예측 후, 같은 비디오 내 프레임의 행동 예측 값의 최빈값을 해당 비디오의 행동 예측 값으로 선정***\n# hint : 위 셀에서 선언한 train_global_label, test_global_id 및 mode 활용\n################################################################################################","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nSVM global voting in frame\")\nstart = time.time()\n\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 0)\nsvm.fit(train_global_desc, train_global_label)\nsvm_predict = svm.predict(test_global_desc)\nsvm_predict_vote = []\nfor i in np.unique(test_global_id):\n    tind = np.where(test_global_id == i)[0]\n    voted = mode(svm_predict[tind]).mode.item() # 최빈값 저장\n    svm_predict_vote.append(voted)\nsvm_predict_vote = np.asarray(svm_predict_vote)\n\nsaveFile(classinfo_arr[svm_predict_vote][:,1], np.arange(len(test_list)), \"svm_global_voting\")\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 프레임 feature에서 대표되는 feature를 선정 후 BoW or VLAD 방식으로 비디오 feature를 기술 (Empty Module 7)","metadata":{}},{"cell_type":"code","source":"train_global_alloc, test_global_alloc, global_codebook, global_kmeans = clustering(train_global_desc, test_global_desc, args_global_cluster)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nAllocate center & Descript global histogram\")\nstart = time.time()\ntrain_vid_names = np.asarray([i.split(\", \")[0] for i in train_global_desc_key])\ntrain_vid_names_u = np.unique(train_vid_names)\n\n# Train 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n# 각 비디오에 대한 비디오 feature 기술\n# Empty Module 7과 관련있으며, 5,6과는 무관\ntrain_video_desc = []\ntrain_video_desc_key = []\nfor vid_name in train_vid_names_u:\n    cind = np.where(vid_name==train_vid_names)[0]\n    if args_aggr==\"bow\":\n        hist_desc = BoW(train_global_alloc[cind], args_global_cluster)\n    elif args_aggr==\"vlad\":\n        hist_desc = VLAD(train_global_desc[cind], train_global_alloc[cind], global_codebook)\n    else:\n        print(\"Set the appropriate variable args_aggr!!\")\n        #import pdb; pdb.set_trace()\n\n    train_video_desc.append(hist_desc)\n    train_video_desc_key.append(vid_name)\ntrain_video_desc = np.asarray(train_video_desc)\ntrain_video_desc_key = np.asarray(train_video_desc_key)\n\n# Test 비디오 내 프레임 별로 기술된 이미지 feature를 기반으로 한번 더 기술하여(한번 더 BoW 혹은 VLAD)\n# 각 비디오에 대한 비디오 feature 기술\n# Empty Module 7과 관련있으며, 5,6과는 무관\ntest_vid_names = np.asarray([i.split(\", \")[0] for i in test_global_desc_key])\ntest_vid_names_u = np.unique(test_vid_names)\n\ntest_video_desc = []\ntest_video_desc_key = []\nfor vid_name in test_vid_names_u:\n    cind = np.where(vid_name==test_vid_names)[0]\n    if args_aggr==\"bow\":\n        hist_desc = BoW(test_global_alloc[cind], args_global_cluster)\n    elif args_aggr==\"vlad\":\n        hist_desc = VLAD(test_global_desc[cind], test_global_alloc[cind], global_codebook)\n    else:\n        print(\"Set the appropriate variable args_aggr!!\")\n        import pdb; pdb.set_trace()\n\n    test_video_desc.append(hist_desc)\n    test_video_desc_key.append(vid_name)\ntest_video_desc = np.asarray(test_video_desc)\ntest_video_desc_key = np.asarray(test_video_desc_key)\n\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nProcessing label\")\nstart = time.time()\n\n# 분류를 위해, 행동 분류에 대한 각 train 비디오 별 label 가공\ntrain_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in train_video_desc_key])\ntrain_video_label = []\nfor fid in train_video_id:\n    cind = np.where(train_csv_arr[:, 0]==fid)[0]\n    clsname = train_csv_arr[cind, 1]\n    cinfo_ind = np.where(classinfo_arr[:, 1] == clsname)[0]\n    train_video_label.append(classinfo_arr[cinfo_ind, 0].astype(\"int\"))\ntrain_video_label = np.asarray(train_video_label).ravel()\n\n# 분류를 위해, 행동 분류에 대한 각 test 비디오 별 id 가공\ntest_video_id = np.array([int(i.split(\"/\")[-1].split(\".\")[0]) for i in test_video_desc_key])\n\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 이미지 feature에 대해 다시 한번 VLAD feature 기술 방식을 사용하여 video feature를 기술한 경우 큰 차원으로 인해 메모리 부족 현상이 발생하므로 PCA를 이용한 차원 축소\nif args_aggr==\"vlad\":\n    print(\"\\n\\nReduce dim of descriptor of the frames with PCA\")\n    start = time.time()\n    pca = PCA(n_components=pca_vlad, random_state=0)\n    pca.fit(train_video_desc)\n    train_video_desc = pca.transform(train_video_desc)\n    test_video_desc = pca.transform(test_video_desc)\n    print(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################## Empty Module 7 : SVM (video feature) ##########################\n# 제약조건 : sklearn의 SVC 사용 및 random_state=0으로 고정\n#          베이스라인 파라미터는 defalut 값을 이용하였습니다\n# 이전 이미지 feature를 기반으로 각 기술방식(BoW, VLAD)을 한번 더 적용해 만든 비디오 feature 사용\n# 비디오 feature로 학습 후, 행동 예측\n#######################################################################################","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\nSVM video descriptor\")\nstart = time.time()\n\nsvm = SVC(random_state=0, probability = True, class_weight = 'balanced')\nsvm.fit(train_video_desc, train_video_label)\nsvm_predict = svm.predict(test_video_desc)\n\nsaveFile(classinfo_arr[svm_predict][:,1], test_video_id, \"svm_video\")\nprint(\"\\t{:3.2f}s\\n\\n\".format(time.time()-start))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}